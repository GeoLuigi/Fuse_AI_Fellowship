{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f9a4ab",
   "metadata": {},
   "source": [
    "# KNN Algorithm\n",
    "\n",
    "<li>The KNearest Neighbor(KNN) algorithm is a type of supervised Learning used for classification and regression</li>\n",
    "<li>This algorithm considers the K closest neighbors to predict the class or continuous value for a new datapoint.</li>\n",
    "<li>KNN is a non parametric and an instance-based learning algorithm.</li>\n",
    "<li>Instead of learning the weights from training data, it memorizes the entire training instances to predict the output for new data.</li>\n",
    "<li>KNN has additional applications such as imputing missing values and resampling datasets.</li>\n",
    "<li>KNN algorithm is considered lazy learning because the learning process is postponed until prediction is requested on the new instance.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a32b31",
   "metadata": {},
   "source": [
    "## How KNN works?\n",
    "\n",
    "<li>The working of KNN is explained in the following steps:</li>\n",
    "<ol>\n",
    "    <li>Compute <b>similarity metric</b> for a new test case from the group of items.</li>\n",
    "    <li>Rank each listing by the similarity metric and select the top <b>k</b> listings with respect to the test case.</li>\n",
    "    <li>Calculate the average of top <b>k</b> items or mode of top n items based on the problem statement and return it as a prediction.</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5fb4f5",
   "metadata": {},
   "source": [
    "## Parameters Of KNN Algorithm\n",
    "\n",
    "<li>There are two things we need to choose prior to applying KNN algorithm. They are:</li>\n",
    "<ol>\n",
    "    <li>Choice of similarity metric</li>\n",
    "    <li>Choosing appropriate value of k</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393e6f41",
   "metadata": {},
   "source": [
    "## Similarity Metric\n",
    "\n",
    "<li>The KNN algorithm offers a variety of similarity metrics (distance metrics) to choose from. Some of them are:</li>\n",
    "<ol>\n",
    "    <li>Minkowski distance</li>\n",
    "    <li>Manhattan distance</li>\n",
    "    <li>Euclidean distance</li>\n",
    "    <li>Cosine similarity</li>\n",
    "    <li>Jaccard similarity</li>\n",
    "<ol>\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38794910",
   "metadata": {},
   "source": [
    "### Minkowski Distance\n",
    "<li>Minkowski distance is a distance/ similarity measurement between two points in the normed vector space (N dimensional real space)</li>\n",
    "<li>It is a generalization of the Euclidean distance and the Manhattan distance.</li>\n",
    "<li>It is given by the formula:</li>\n",
    "\n",
    "![](images/minoski_distance.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a5ec6a",
   "metadata": {},
   "source": [
    "### Manhattan Distance\n",
    "\n",
    "<li>Manhattan distance is a distance measure that is calculated by taking the sum of distances between the x and y coordinates.</li>\n",
    "<li>We can derive the formula to compute manhattan distance from minkowski distance formula.</li>\n",
    "<li>When we substitute the value for p=1, then we get the formula to compute manhattan distance.</li>\n",
    "<li>The formula to compute Manhattan distance is given by:</li>\n",
    "\n",
    "![](images/manhattan_dist.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0648f8f",
   "metadata": {},
   "source": [
    "### Euclidean Distance\n",
    "<li>Euclidean distance is a distance measure that is calculated by taking the square root of the sum of squared distances between the x and y coordinates.</li>\n",
    "<li>We can also derive the formula to compute euclidean distance from minkowski distance formula.</li>\n",
    "<li>When we substitute the value for p=2, then we get the formula to compute euclidean distance.</li>\n",
    "<li>The formula to compute Euclidean distance is given by:</li>\n",
    "\n",
    "![](images/euclidean_dist.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5590bd17",
   "metadata": {},
   "source": [
    "### How To Choose Appropriate Value Of K\n",
    "\n",
    "<li>K is a crucial parameter in the KNN algorithm.Some suggestions for choosing K Value are:</li>\n",
    "<li>K value should be odd while considering binary(two-class) classification.</li>\n",
    "<li>There are no pre-defined statistical methods to find the appropriate value of K.</li>\n",
    "<li>Initialize a random K value and start computing.</li>\n",
    "<li>Choosing a small value of K leads to unstable decision boundaries.</li>\n",
    "<li>The optimal K value usually found is the square root of N, where N is the total number of samples.</li>\n",
    "<li>The substantial K value is better for classification as it leads to smoothening the decision boundaries.</li>\n",
    "<li>Domain knowledge can be very useful in choosing the K value.</li>\n",
    "<li>Derive a plot between error and K denoting values in a defined range. Then choose the K value as having a minimum error.</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fe9448",
   "metadata": {},
   "source": [
    "### Data Preparation For Applying KNN\n",
    "\n",
    "<b>1. Feature Scaling:</b>\n",
    "<li>The first step in data preparation is to perform data scaling.</li>\n",
    "<li>This is done to ensure that all features are on the same scale, which is important for locating data points in a multidimensional feature space.</li>\n",
    "<li>Normalization or standardization techniques can be used to achieve this goal.</li>\n",
    "<li>By scaling the data, it becomes easier to compare and analyze the features and ensure that the KNN algorithm can accurately identify the nearest neighbors.</li>\n",
    "\n",
    "<b>2. Dimensionality Reduction:</b>\n",
    "<li>KNN may not work well if there are too many features.</li>\n",
    "<li>Hence dimensionality reduction techniques like feature selection, principal component analysis can be implemented.</li>\n",
    "\n",
    "<b>3. Missing value treatment:</b>\n",
    "<li>If out of M features one feature data is missing for a particular example in the training set then we cannot locate or calculate distance from that point.</li>\n",
    "<li>Therefore deleting that row or imputation is required.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e56d70b",
   "metadata": {},
   "source": [
    "### KNN For Classification\n",
    "\n",
    "<li>Suppose, we have a binary classification problems with two class; <b>Class A(red star)</b> and <b>Class B(green triangle)</b>.</li>\n",
    "<li>Let us suppose, we wanted to classify a new data point which is represented as <b>'?'</b> in the figure.</li>\n",
    "\n",
    "\n",
    "![](images/knn_classifier1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cdbd375",
   "metadata": {},
   "source": [
    "![](images/knn_classifier2.png)\n",
    "\n",
    "<li>Then we will compute the distance from the new data point to all possible data points we have in our training set.</li>\n",
    "<li>We can compute any distance metrics; euclidean or manhattan distance and then rank the similarity based on their distance metrics.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b33ef8b",
   "metadata": {},
   "source": [
    "![](images/knn_classifier3.png)\n",
    "\n",
    "<li>Once they are ranked, we will choose any value of <b>'k'</b>.</li>\n",
    "<li>Here, <b>k</b> denotes the number of nearest neigbours we are going to look for inorder to predict the new test data.</li>\n",
    "<li>If <b>k=3</b> then, we will look into the 3 nearest neighbors with the smallest distance to the test data.</li>\n",
    "<li>Then we will assign the newly test data with a class which occurs most often among the nearest neighbors.</li>\n",
    "<li>In the figure, out of 3 nearest neighbors, two of them belong to 'Class B' so we assign the newly test data with 'Class B'.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175c024a",
   "metadata": {},
   "source": [
    "![](images/knn_classifier4.png)\n",
    "\n",
    "<li>But when k=7, we will look for 7 nearest neighbors inorder to classify the new test data.</li>\n",
    "<li>In the figure, out of 4 nearest neighbors, 4 of them belong to 'Class A' and 3 of them belong to 'Class B'.</li>\n",
    "<li>Based on majority vote counting, we assign the new test data with 'Class A'.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ae47512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45746c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "642514d2",
   "metadata": {},
   "source": [
    "### KNN For Regression\n",
    "\n",
    "<li>Incase of regression, the process is same untill the prediction step.</li>\n",
    "<li>Like classification, we will first compute distance similarity metric for the new test data against all training data.</li>\n",
    "<li>Then, we will rank all these distance metrics and based on the value of k we will choose first k values with the lowest distance.</li>\n",
    "<li>Since our target value is continuous incase of regression so instead of predicting the class we need to predict a value.</li>\n",
    "<li>So, for regression, we will predict the average of the top k values and return it as a prediction.</li>\n",
    "\n",
    "![](images/knn_regressor.png)\n",
    "\n",
    "<li>As shown in the above figure, we predict the new data point by computing the average of the top k nearest values(3) in case of example.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d0a7904",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c5661bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4edd0b77",
   "metadata": {},
   "source": [
    "### Advantages Of KNN Algorithm\n",
    "\n",
    "\n",
    "<li>Simple to understand and easy to implement.</li>\n",
    "<li>Non-parametric model that does not make any assumptions about the underlying data distribution.</li>\n",
    "<li>Can be used for both classification and regression tasks.</li>\n",
    "<li>Works well with small datasets and noisy data.</li>\n",
    "<li>Does not require training time, as the algorithm only needs to store/memorize the training data.</li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297a7b68",
   "metadata": {},
   "source": [
    "### Disadvantages of KNN algorithm:\n",
    "\n",
    "<li>Computationally expensive and slow, especially for large datasets.</li>\n",
    "<li>Requires a lot of memory to store the entire dataset.</li>\n",
    "<li>Sensitivity to the choice of the distance metric used to measure similarity between data points.</li>\n",
    "<li>KNN can be affected by the curse of dimensionality, where the performance decreases as the number of features increases.</li>\n",
    "<li>KNN is not effective for imbalanced datasets where the number of examples in each class is significantly different.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d44ec04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
