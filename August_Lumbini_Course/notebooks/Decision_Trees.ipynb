{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28226f73",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "<li>Decision Trees (DTs) are a non-parametric supervised learning method used for classification and regression.</li>\n",
    "<li>It is a non-parametric learning algorithm because it doesnot make any assumptions about the underlying data distribution or parameters.</li>\n",
    "<li>The goal of decision trees is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.</li>\n",
    "<li>Decision Tree has a hierarchical, tree like structure, which consists of a root node, branches, internal nodes and leaf nodes.</li>\n",
    "\n",
    "![](images/decision_trees.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9977bf",
   "metadata": {},
   "source": [
    "## How Decision Trees Work?\n",
    "<li>The decision tree algorithm builds the tree in a recursive way, by selecting the best attribute to split the data at each node based on some criterion.</li>\n",
    "<li>The criterion that can be used for splitting up a decision node is information gain or Gini impurity.</li>\n",
    "<li>Information gain measures the reduction in entropy (i.e., uncertainty) of the class labels after a split.</li>\n",
    "<li>Entropy is defined as a measure of randomness or disorder of a system.</li>\n",
    "<li>Information gain and Entropy are inversely proportional to each other.</li>\n",
    "<li>When entropy increases, information gain decreases and when entropy decreases, information gain increases.</li>\n",
    "<li>Gini impurity measures the probability of misclassifying a random sample from the node.</li>\n",
    "<li>The process continues until all the instances in a node belong to the same class or until a stopping criterion is met.</li>\n",
    "<li>Stopping criterion could be maximum tree depth or minimum number of instances per leaf.</li>\n",
    "<li>The resulting tree can be used to classify new instances by traversing from the root to a leaf node, following the path that satisfies the tests at each node.</li>\n",
    "\n",
    "![](images/working_of_dtrees.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e48841",
   "metadata": {},
   "source": [
    "## Decision Tree Inducers (Types Of Decision Tree Algorithm)\n",
    "<li>A decision tree inducer is an algorithm that is used to build a decision tree from a given dataset. Here are some commonly used decision tree inducers:</li>\n",
    "<ol>\n",
    "    <b><li>ID3</li></b>\n",
    "    <b><li>C4.5</li></b>\n",
    "    <b><li>CART</li></b>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4809168",
   "metadata": {},
   "source": [
    "<b>1. ID3:</b>\n",
    "<li>The full form of ID3 algorithm is Iterative Dichotomiser 3.</li>\n",
    "<li>This is one of the earliest decision tree algorithms developed by Ross Quinlan.</li> \n",
    "<li>It uses the concept of entropy and information gain to select the best attribute for splitting the data at each node.</li>\n",
    "<li>It cannot handle numeric featues and it can only be used for classification tasks only.</li>\n",
    "\n",
    "<b>2. C4.5:</b>\n",
    "<li>C4.5 is actually an abbreviation for \"Classifier Version 4.5\".</li>\n",
    "<li>It is a decision tree algorithm that was developed by Ross Quinlan, and it is an extension of the earlier ID3 algorithm.</li>\n",
    "<li>The C4.5 algorithm can handle both discrete and continuous data.</li>\n",
    "<li>It uses <b>information gain ratio</b> as the splitting criterion.</li>\n",
    "<li>It also includes a post-pruning step to reduce overfitting.</li>\n",
    "\n",
    "<b>3. CART:</b>\n",
    "<li>The full form of CART is Classification And Regression Trees.</li>\n",
    "<li>This is a decision tree algorithm developed by Breiman, Friedman, Olshen, and Stone.</li>\n",
    "<li>It can be used for both classification and regression tasks.</li>\n",
    "<li>It uses the GIni impurity measure to select the best attribute for splitting the data.</li>\n",
    "\n",
    "![](images/decision_tree_inducers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd99e7d",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "<li>We use the concept of Entropy and Information Gain while splitting up a node in an ID3 algorithm.</li>\n",
    "<li>Entropy is defined as a measure of randomness or disorder in the system.</li>\n",
    "<li>The formula to calculate entropy is given by:</li>\n",
    "\n",
    "![](images/Entropy_formula.png)\n",
    "\n",
    "<li>Here, c is the number of class. So for binary classification problem, the entropy formula is given by:</li>\n",
    "\n",
    "![](images/expanded_eqn_entropy.png)\n",
    "\n",
    "<li>Here, p is the probablity that it belongs to positive class and q is the probability that it belongs to negative class.</li>\n",
    "<li>Let's say you are predicting whether the employee will get a promotion or not.</li>\n",
    "<li>If only 30% of employees in your total dataset has received promotion then your p=0.3 being your positive class and q=1-p=0.7 being your negative class.</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26e5705",
   "metadata": {},
   "source": [
    "## Information Gain & Splitting Of Node In ID3 Algorithm\n",
    "<li>One of the key steps in ID3 algorithm is to split a node into child nodes based on the attribute that maximizes the information gain.</li>\n",
    "\n",
    "<li>Information gain is a measure of the reduction in entropy (impurity) of the dataset after splitting the data based on an attribute.</li>\n",
    "<li>Entropy is a measure of the randomness or uncertainty in the dataset.</li>\n",
    "\n",
    "**The formula for information gain is:**\n",
    "<code>\n",
    "Information Gain = Entropy(parent) - âˆ‘ [Weighted Average] * Entropy(children)\n",
    "</code>\n",
    "**where**\n",
    "\n",
    "<li>Entropy(parent) is the entropy of the parent node</li>\n",
    "<li>Entropy(children) is the entropy of each child node</li>\n",
    "<li>the Weighted Average is the proportion of the data that belongs to each child node.</li>\n",
    "\n",
    "\n",
    "![](images/information_gain_id3.png)\n",
    "<li>Firstly, we calculate the entropy of the parent node\n",
    "<li>After calculating entropy, we calculate the information gain for each of the attributes.</li>\n",
    "<li>The attribute that results in the highest information gain is selected as the splitting attribute for the node.</li> \n",
    "<li>The node is then split into child nodes based on the values of the selected attribute.</li>\n",
    "<li>This process is repeated recursively until all leaf nodes are pure (contain only one class) or until some stopping criteria is met.</li>\n",
    "<li>In this way, ID3 algorithm uses information gain to select the attribute to split a node and to construct a decision tree from the dataset.</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fa33e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9af2750",
   "metadata": {},
   "outputs": [],
   "source": [
    "gameplay_df = pd.DataFrame({\n",
    "    \"Outlook\": [\"Sunny\", \"Sunny\", \"Overcast\", \"Rain\", \"Rain\",\n",
    "               \"Rain\", \"Overcast\", \"Sunny\", \"Sunny\", \"Rain\",\n",
    "               \"Sunny\", \"Overcast\", \"Overcast\", \"Rain\", \n",
    "               \"Sunny\", \"Overcast\", \"Rain\"],\n",
    "    \"Temperature\": [\"Hot\", \"Hot\", \"Hot\", \"Mild\", \"Cool\",\n",
    "                   \"Cool\", \"Cool\", \"Mild\", \"Cool\", \"Mild\",\n",
    "                   \"Mild\", \"Mild\", \"Hot\", \"Mild\",\n",
    "                   \"Hot\", \"Mild\", \"Cool\"],\n",
    "    \"Humidity\": [\"High\", \"High\", \"High\", \"High\", \"Normal\",\n",
    "                \"Normal\", \"Normal\", \"High\", \"Normal\", \"Normal\",\n",
    "                \"Normal\", \"High\", \"Normal\", \"High\",\n",
    "                \"Normal\", \"High\", \"Normal\"],\n",
    "    \"Wind\": [\"Weak\", \"Strong\", \"Weak\", \"Weak\", \"Weak\",\n",
    "            \"Strong\", \"Strong\", \"Weak\", \"Weak\", \"Weak\",\n",
    "            \"Strong\", \"Strong\", \"Weak\", \"Strong\", \n",
    "            \"Strong\", \"Weak\", \"Strong\"],\n",
    "    \"Play\" : [\"No\", \"No\", \"Yes\", \"Yes\", \"Yes\",\n",
    "             \"No\", \"Yes\", \"No\", \"Yes\", \"Yes\",\n",
    "             \"Yes\", \"Yes\", \"Yes\", \"No\",\n",
    "             \"Yes\", \"Yes\", \"No\"]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db8f01c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Outlook</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Wind</th>\n",
       "      <th>Play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Mild</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Sunny</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Overcast</td>\n",
       "      <td>Mild</td>\n",
       "      <td>High</td>\n",
       "      <td>Weak</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Rain</td>\n",
       "      <td>Cool</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Strong</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Outlook Temperature Humidity    Wind Play\n",
       "0      Sunny         Hot     High    Weak   No\n",
       "1      Sunny         Hot     High  Strong   No\n",
       "2   Overcast         Hot     High    Weak  Yes\n",
       "3       Rain        Mild     High    Weak  Yes\n",
       "4       Rain        Cool   Normal    Weak  Yes\n",
       "5       Rain        Cool   Normal  Strong   No\n",
       "6   Overcast        Cool   Normal  Strong  Yes\n",
       "7      Sunny        Mild     High    Weak   No\n",
       "8      Sunny        Cool   Normal    Weak  Yes\n",
       "9       Rain        Mild   Normal    Weak  Yes\n",
       "10     Sunny        Mild   Normal  Strong  Yes\n",
       "11  Overcast        Mild     High  Strong  Yes\n",
       "12  Overcast         Hot   Normal    Weak  Yes\n",
       "13      Rain        Mild     High  Strong   No\n",
       "14     Sunny         Hot   Normal  Strong  Yes\n",
       "15  Overcast        Mild     High    Weak  Yes\n",
       "16      Rain        Cool   Normal  Strong   No"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gameplay_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bcf994",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777178de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5cd1872f",
   "metadata": {},
   "source": [
    "### Gini Index \n",
    "<li>Gini index is a measure of impurity or diversity used to select the best split in decision trees.</li>\n",
    "<li>In the context of decision trees, it is used to measure the quality of a split when determining the feature that should be used to create child nodes.</li>\n",
    "<li>The main goal of measuring impurity is to create child nodes that are as pure as possible in terms of the target variable.</li>\n",
    "<li>The Gini index measures the probability of misclassifying a randomly chosen element from a dataset.</li>\n",
    "<li>It ranges from 0 to 1, where 0 indicates a pure node and 1 indicates maximum impurity.</li>\n",
    "<li>A pure node is a node where all elements belong to the same class.</li>\n",
    "<li>An impure node is a node where elements are equally distributed across all classes.</li>\n",
    "\n",
    "**The formula for calculating the Gini index for a leaf node is:**\n",
    "<code>\n",
    "Gini Index(Leaf) = 1 - âˆ‘(p_i^2)\n",
    "</code>\n",
    "\n",
    "**where p_i is the proportion of samples that belong to class i in the node.**\n",
    "\n",
    "<li>After calculating the gini index for a leaf node, weighted gini index for the node is calculated based on the formula.</li>\n",
    "\n",
    "![](images/weighted_gini_index.png)\n",
    "\n",
    "<li>When selecting a split in a decision tree, the feature that results in the lowest weighted Gini index (highest purity) is chosen.</li>\n",
    "<li>The resulting split divides the dataset into two or more child nodes, which are then processed recursively to create the decision tree.</li>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe5367",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b38e97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4891905a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9c31d785",
   "metadata": {},
   "source": [
    "### Decision Tree For Regression\n",
    "<li>For classification, DT tries to split node by maximizing information gain incase of ID3 or minimizing gini index incase of CART.</li>\n",
    "<li>But for regression, the goal is to reduce the variance of the target variable (i.e., the dependent variable).</li>\n",
    "<li>Decision Trees works on the principle of variance reduction since the target variable is continuous.</li>\n",
    "<li>This is typically done by minimizing the sum of squared differences between the target variable and the mean value of the samples in each resulting group.</li>\n",
    "\n",
    "### How Splitting Of Node is Done in Decision Tree Regressor\n",
    "\n",
    "<li>The decision tree regressor considers all possible splits for each predictor variable and selects the one that maximizes the variance reduction.</li>\n",
    "<li>The process is repeated recursively for each resulting group until a stopping criterion is met.</li>\n",
    "<li>Common stopping criteria include a minimum number of samples required to split a node, a maximum tree depth.</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aca6a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dacc5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61882c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a54fa1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
